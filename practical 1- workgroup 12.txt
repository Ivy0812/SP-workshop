#This work is completed by a group of three:
#Yenyi Yu s2161093
#Minke Pan s2160782
#Yanren Mao s2207399
setwd("/Users/yuwenyi/SP-workshop")
a <- scan("1581-0.txt",what="character",skip=156)
n <- length(a)
a <- a[-((n-2909):n)] ## strip license

#step 4 
#create a function with input of punctuation marks
split_punct <- function(p) { 
ip <- grep(p, a, fixed=TRUE) #find indices of words containing punctuation marks, p
xa <- gsub(p,"",a,fixed=TRUE) #substitute variable p with “”
xxa <- rep("", length(xa) + length(ip)) #create an empty vector to store individual digits
iia <- ip+1:length(ip) #compute locations for punctuation marks
xxa[iia] <- paste(p) #assign punctuation marks
xxa[-iia] <- xa #insert the rest units
return(xxa)
}

#step 5
#Separate punctuation marks
punc <- c(",", ".", ";", "!", ":", "?")#create a vector containing punctuation marks
for (p in punc) {
  a <- split_punct(p)
}


#step 6
aa <- tolower(a) #replace capital letters with lower case letters
b <- unique(aa) #find the vector of unique words
c <- match(aa,b) #find indices representing where every unique word in aa is located in b
freq <- tabulate(c) #indicate the number of times each unique word occurs in the text
freq_1000 <- sort(freq,decreasing=TRUE) #find the vector of frequency in deceasing order 
order_index <- order(freq[freq >= freq_1000[1000]], decreasing = TRUE) #create the vector of indices whose frequency is greater than or equal to the 1000th
b_index <- sort(order_index) #sort by increasing order
b <- b[b_index] #create a vector b containing m most common words

#STEP 7
d <- match(aa,b) #indices of bible words in common word vector b
D <- cbind(d[-length(aa)], d[2:length(aa)]) #remove the first and last entries
D <- D[-which(is.na(rowSums(D)) == TRUE),] #matrix storing indices of common word pairs
A <- matrix(0,length(b),length(b)) #initializing transition probility matrix A to be inserted
for (i in 1:dim(D)[1]){
  A[D[i,1],D[i,2]] = A[D[i,1],D[i,2]] + 1
} #count the number of each common word pair
for (i in 1:length(b)){ #standardlize every row，dividing by the sum of all entries in the row
  A[i,] <- A[i,]/rowSums(A)[i]
}

#step 9
cap_index <- grep("\\b^(?=[A-Z])", a, perl = TRUE) #find the indices or words starting with capital letters
cap_word <- a[cap_index] #vector of words starting with capital letters
low_word <- tolower(cap_word) #vector storing lower case version of cap_word
uniq <- unique(low_word) #find the vector of unique words
b_uniq <- match(b,uniq) #find indices of common words
b_uniq <- b_uniq[-which(is.na(b_uniq))]
low_overlap <- uniq[b_uniq] #overlapping words with lower case letters
cap_overlap <- unique(cap_word[match(low_overlap,low_word)]) #find the capital case version of overlapping words
cap_freq <-tabulate(match(a, cap_overlap)) #the frequency of words with capital letters in a
low_freq <-tabulate(match(a, low_overlap)) #the frequency of words with lower case letters in a

#replace words with lower case letters if the frequency of them is smaller than words starts with capital words
for (i in 1:length(cap_freq)){
  if (cap_freq[i] >= low_freq[i]){
    inn <- match(low_overlap[i],b)
    b[inn] = cap_overlap[i]
  }
}


#step 8
set.seed(999)
wsim = rep("", times = 50) #initializing 50 simulation words vector
isim = rep(0,times = 50) #initializing indices for 50 simulation words vector
wsim[1] <- sample(b,1) #randomly choose the first word from b
isim[1] = which(b == wsim[1]) #identify its index in b
for (i in 1:49){
  isim[i+1] = sample(1:length(b), 1, replace = TRUE, prob = A[isim[i],])
  wsim[i+1] = b[isim[i+1]]
} #generating sequence of words by sampling from pdf P(|i=isim[i])
cat(wsim) 
